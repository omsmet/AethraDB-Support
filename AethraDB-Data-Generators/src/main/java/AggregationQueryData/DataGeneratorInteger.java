package AggregationQueryData;

import org.apache.arrow.memory.BufferAllocator;
import org.apache.arrow.memory.RootAllocator;
import org.apache.arrow.vector.IntVector;
import org.apache.arrow.vector.VectorSchemaRoot;
import org.apache.arrow.vector.ipc.ArrowFileWriter;
import org.apache.arrow.vector.types.pojo.ArrowType;
import org.apache.arrow.vector.types.pojo.Field;
import org.apache.arrow.vector.types.pojo.FieldType;
import org.apache.arrow.vector.types.pojo.Schema;
import org.apache.commons.math3.distribution.ZipfDistribution;
import org.apache.commons.math3.random.RandomGenerator;

import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.util.Arrays;
import java.util.Random;

import static java.util.Arrays.asList;
import static org.apache.commons.math3.genetics.GeneticAlgorithm.getRandomGenerator;

/**
 * This data generator is aimed at reproducing datasets which are similar to the PCQ paper.
 * (https://linmagit.github.io/publications/2020.pcq.vldb.pdf).
 * However, since the exact construction of the dataset is unavailable, we make some assumptions.
 * These are noted below whenever applicable.
 * There are two dataset types to generate: one which has a varying number of aggregations in the
 * query, and one which has a varying skew in how many records exist per key.
 */
public class DataGeneratorInteger {

    /**
     * Standard path prefix for any dataset generated by this generator.
     */
    public static final String targetDirectoryPrefix = "/nvtmp/AethraTestData/aggregation_query_int/arrow";

    /**
     * The size of the generated datasets.
     * The paper does not mention the number of rows in the actual dataset, so we assume / decide
     * that this number is equal to the provided value.
     */
    public static final int datasetSize = 3 * 1024 * 1024 * 10;

    /**
     * Random source for generating the datasets.
     */
    private static final Random rg = new Random(140623);

    /**
     * The length of vectors in the resulting arrow files.
     */
    public static final int vectorLength = 16384;

    /**
     * Method which simply invokes the generators for each dataset type.
     * @param args The arguments with which the method was invoked. These are ignored.
     */
    public static void main(String[] args) {
        generateVaryingAggregates();
        generateSkewedKeys();
    }

    /**
     * Method which generates all datasets of the type which has a varying number of keys.
     */
    private static void generateVaryingAggregates() {
        // The number of keys in different datasets are just as in the paper
        int[] keySetSizes = new int[] {
                1, // 2^0
                2, // 2^1
                2 << 2,
                2 << 3,
                2 << 4,
                2 << 5,
                2 << 6,
                2 << 7,
                2 << 8,
                2 << 9,
                2 << 10,
                2 << 11,
                2 << 12,
                2 << 13,
                2 << 14,
                2 << 15,
                2 << 16,
                2 << 17,
                2 << 18
        };

        // Now, we generate each dataset
        for (int keySetSize : keySetSizes) {
            String datasetFolderName = targetDirectoryPrefix + "_size_" + datasetSize + "_keys_" + keySetSize;
            File datasetFolder = new File(datasetFolderName);

            if (datasetFolder.exists())
                throw new RuntimeException("The folder " + datasetFolderName + " already exists. Stopping ...");

            if (!datasetFolder.mkdir())
                throw new RuntimeException("Could not create folder " + datasetFolderName);

            generateVaryingAggregate(datasetFolder, keySetSize, datasetSize);
        }
    }

    /**
     * Method for generating the dataset type which has a specific number of keys in the aggregation.
     * The keys should not have any skew, and should be uniformly distributed.
     * @param targetFolder The folder in which the generated dataset should be stored.
     * @param numberOfKeys The number of keys to be contained in the table.
     * @param numberOfRecords The number of records that should be present in the table.
     */
    private static void generateVaryingAggregate(File targetFolder, int numberOfKeys, int numberOfRecords) {
        // Create the target file object
        File targetFile = new File(targetFolder, "aggregation_query_table.arrow");

        // First we need to generate a set of unique keys (we assume these are non-negative integers)
        int[] keys = generateKeys(numberOfKeys);

        // Generate the records and store them in an arrow file
        // Get an allocator to allocate the vectors
        try (BufferAllocator allocator = new RootAllocator()) {
            // Define the schema
            Field col1Field = new Field("col1", FieldType.notNullable(new ArrowType.Int(32, true)), null);
            Field col2Field = new Field("col2", FieldType.notNullable(new ArrowType.Int(32, true)), null);
            Field col3Field = new Field("col3", FieldType.notNullable(new ArrowType.Int(32, true)), null);
            Field col4Field = new Field("col4", FieldType.notNullable(new ArrowType.Int(32, true)), null);

            Schema aggregationQuerySchema = new Schema(asList(col1Field, col2Field, col3Field, col4Field));

            // Write the arrow file to disk
            try (
                    VectorSchemaRoot root = VectorSchemaRoot.create(aggregationQuerySchema, allocator);
                    FileOutputStream fileOutputStream = new FileOutputStream(targetFile);
                    ArrowFileWriter writer = new ArrowFileWriter(root, null, fileOutputStream.getChannel())
            ) {
                // Initialise the writer
                writer.start();

                // Initialise the vectors for the batches
                IntVector col1Vector = (IntVector) root.getVector("col1");
                IntVector col2Vector = (IntVector) root.getVector("col2");
                IntVector col3Vector = (IntVector) root.getVector("col3");
                IntVector col4Vector = (IntVector) root.getVector("col4");

                // Initialise the read pointer into the columns
                int currentPosition = 0;

                // Create the batches
                while (currentPosition < numberOfRecords) {
                    int trueVectorLength = Math.min(vectorLength, numberOfRecords - currentPosition);

                    col1Vector.reset();
                    col2Vector.reset();
                    col3Vector.reset();
                    col4Vector.reset();
                    col1Vector.allocateNew(trueVectorLength);
                    col2Vector.allocateNew(trueVectorLength);
                    col3Vector.allocateNew(trueVectorLength);
                    col4Vector.allocateNew(trueVectorLength);

                    for (int i = 0; i < trueVectorLength; i++) {
                        // Pick a random key from the set
                        int keyIndex = rg.nextInt(numberOfKeys);
                        col1Vector.set(i, keys[keyIndex]);

                        // Generate the remaining values randomly in their domain
                        col2Vector.set(i, rg.nextInt());
                        col3Vector.set(i, rg.nextInt());
                        col4Vector.set(i, rg.nextInt());
                    }

                    col1Vector.setValueCount(trueVectorLength);
                    col2Vector.setValueCount(trueVectorLength);
                    col3Vector.setValueCount(trueVectorLength);
                    col4Vector.setValueCount(trueVectorLength);
                    root.setRowCount(trueVectorLength);

                    writer.writeBatch();

                    currentPosition += trueVectorLength;
                }

                writer.end();
            } catch (IOException e) {
                System.out.println("Could not successfully construct " + targetFile);
            }
        }

    }

    /**
     * Method which generates all skewed-key datasets.
     */
    private static void generateSkewedKeys() {
        // As in the paper, for this kind of dataset, we fix the number of keys at ~200k
        int[] keys = generateKeys(2 << 18);

        // Then, we generate a dataset with a skewed key distribution using a zipfian distribution
        // with varying skew. We assume that the paper uses the following skew parameters
        // (judging by figure 10)
        double[] skewFactors = new double[] {
                0.2,
                0.4,
                0.6,
                0.8,
                1.0,
                1.2,
                1.4,
                1.6,
                1.8,
                2.0,
                2.2,
                2.4
        };

        // We generate a dataset for each skew factor
        for (double skewFactor : skewFactors) {
            String datasetFolderName = targetDirectoryPrefix + "_size_" + datasetSize + "_keys_" + keys.length + "_skew_" + skewFactor;
            File datasetFolder = new File(datasetFolderName);

            if (datasetFolder.exists())
                throw new RuntimeException("The folder " + datasetFolderName + " already exists. Stopping ...");

            if (!datasetFolder.mkdir())
                throw new RuntimeException("Could not create folder " + datasetFolderName);

            generateSkewedKey(datasetFolder, skewFactor, keys, datasetSize);
        }
    }

    /**
     * Method which generates a dataset where the distribution of rows per key is skewed according
     * to a skewed zipfian distribution.
     * (https://jasoncrease.medium.com/zipf-54912d5651cc)
     * @param targetFolder The folder in which the generated dataset should be stored.
     * @param skewFactor The skew factor to use in the zipfian distribution.
     * @param keys The key-set to use in the experiment.
     * @param numberOfRecords The number of records that should be present in the table.
     */
    private static void generateSkewedKey(File targetFolder, double skewFactor, int[] keys, int numberOfRecords) {
        // Create the target file object
        File targetFile = new File(targetFolder, "aggregation_query_table.arrow");

        // Initialise the zipfian distribution
        RandomGenerator randomGenerator = getRandomGenerator();
        double seed = (skewFactor + 1) * 140623;
        randomGenerator.setSeed((int) seed);
        ZipfDistribution keyDistribution = new ZipfDistribution(randomGenerator, keys.length, skewFactor);

        // Generate the records and store them in an arrow file
        // Get an allocator to allocate the vectors
        try (BufferAllocator allocator = new RootAllocator()) {
            // Define the schema
            Field col1Field = new Field("col1", FieldType.notNullable(new ArrowType.Int(32, true)), null);
            Field col2Field = new Field("col2", FieldType.notNullable(new ArrowType.Int(32, true)), null);
            Field col3Field = new Field("col3", FieldType.notNullable(new ArrowType.Int(32, true)), null);
            Field col4Field = new Field("col4", FieldType.notNullable(new ArrowType.Int(32, true)), null);

            Schema aggregationQuerySchema = new Schema(asList(col1Field, col2Field, col3Field, col4Field));

            // Write the arrow file to disk
            try (
                    VectorSchemaRoot root = VectorSchemaRoot.create(aggregationQuerySchema, allocator);
                    FileOutputStream fileOutputStream = new FileOutputStream(targetFile);
                    ArrowFileWriter writer = new ArrowFileWriter(root, null, fileOutputStream.getChannel())
            ) {
                // Initialise the writer
                writer.start();

                // Initialise the vectors for the batches
                IntVector col1Vector = (IntVector) root.getVector("col1");
                IntVector col2Vector = (IntVector) root.getVector("col2");
                IntVector col3Vector = (IntVector) root.getVector("col3");
                IntVector col4Vector = (IntVector) root.getVector("col4");

                // Initialise the read pointer into the columns
                int currentPosition = 0;

                // Create the batches
                while (currentPosition < numberOfRecords) {
                    int trueVectorLength = Math.min(vectorLength, numberOfRecords - currentPosition);

                    col1Vector.reset();
                    col2Vector.reset();
                    col3Vector.reset();
                    col4Vector.reset();
                    col1Vector.allocateNew(trueVectorLength);
                    col2Vector.allocateNew(trueVectorLength);
                    col3Vector.allocateNew(trueVectorLength);
                    col4Vector.allocateNew(trueVectorLength);

                    for (int i = 0; i < trueVectorLength; i++) {
                        // Pick a skewed key from the key set
                        int keyIndex = keyDistribution.sample() - 1;
                        col1Vector.set(i, keys[keyIndex]);

                        // Generate the remaining values randomly in their domain
                        col2Vector.set(i, randomGenerator.nextInt());
                        col3Vector.set(i, randomGenerator.nextInt());
                        col4Vector.set(i, randomGenerator.nextInt());
                    }

                    col1Vector.setValueCount(trueVectorLength);
                    col2Vector.setValueCount(trueVectorLength);
                    col3Vector.setValueCount(trueVectorLength);
                    col4Vector.setValueCount(trueVectorLength);
                    root.setRowCount(trueVectorLength);

                    writer.writeBatch();

                    currentPosition += trueVectorLength;
                }

                writer.end();
            } catch (IOException e) {
                System.out.println("Could not successfully construct " + targetFile);
            }
        }

    }

    /**
     * Method to generate a set of unique keys.
     * @param numberOfKeys The number of keys to generate.
     * @return An array containing {@code numberOfKeys} unique, non-negative keys.
     */
    private static int[] generateKeys(int numberOfKeys) {
        int[] keys = new int[numberOfKeys];

        // Generate enough keys
        for (int k = 0; k < keys.length; k++) {
            boolean notUnique = true;
            int key;

            // Generate a new unique value
            generatingLoop: while (true) {
                key = rg.nextInt();
                if (key < 0)
                    continue;

                for (int i = 0; i < k; i++) {
                    if (keys[i] == key)
                        continue generatingLoop;
                }

                break;
            }

            // Store the new unique key
            keys[k] = key;
        }

        // Check keys are unique
        if (Arrays.stream(keys).distinct().toArray().length != numberOfKeys)
            throw new RuntimeException("Generated keyset is not unique!");

        return keys;
    }

}
